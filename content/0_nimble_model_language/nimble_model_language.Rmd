---
title: "NIMBLE's model language"
subtitle: "TWS 2024 Workshop"
author: "NIMBLE Development Team"
date: "October 2024"
output:
  slidy_presentation: default
  beamer_presentation: default
---

<style>
slides > slide {
  overflow-x: auto !important;
  overflow-y: auto !important;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
library(nimble)
library(coda)
recalculate <- FALSE
```

Overview
=====

This module covers the basics of writing models in NIMBLE's model language.

The model language is a dialect of the BUGS language, which was also adopted by JAGS.

Later workshop modules will assume familiarity with the model language.

Resources:

- Our [Documentation](https://r-nimble.org/documentation-2) page, which links to our User Manual, our GitHub workshop repositories, and a variety of papers and other links.

A simple linear regression model
=====

Here is some model code:

```{r, eval=recalculate}
library(nimble)
linear_model_code <- nimbleCode({
  intercept ~ dnorm(0, sd = 100) # vague priors
  slope ~ dnorm(mean = 0, sd = 100)
  sigma ~ dunif(min = 0, max = 100)
  for(i in 1:N) {
    predicted_y[i] <- intercept + slope*x[i]
    y[i] ~ dnorm(predicted_y[i], sd = sigma)
  }
})
```

- Each line of code is a **declaration** of a relationship among **nodes**.
- `<-` declares a **deterministic node**.
- `~` declares a **stochastic nodes**.
- `dnorm` and `dunif` are for normal and uniform distributions.
- *The order of the code does not matter!* `nimble` will figure out the order of calculations from seeing what depends on what. (This is called a *declarative* language; most languages are *imperative*, meaning the code is an ordered sequence of commans.)
- One could instead use:

```{r, eval=FALSE}
nimbleCode({
  # Incomplete code snippet
  for(i in 1:N) {
    y[i] ~ dnorm(intercept + slope*x[i], sd = sigma)
  }
})
```

(Internally, nimble will turn the second version into the first version.)

Building a model object from the code
=====
The mode code is only that: code.

To turn code into an object, we need to provide some combination of:

- `constants` (like `N` or possibly `x`)
- `data` (like `y` or possibly `x`)
- `inits` (initial values for any parameters)

Here is an example:
```{r, eval=recalculate}
set.seed(1) # make this reproducible
x <- rnorm(10)
y <- rnorm(10)
linear_model <- nimbleModel(linear_model_code,
                            constants = list(N = 10),
                            data = list(y = y, x = x))
linear_model$y
```

Simulating and fitting data
=====

Most of this module is about ways to write model code. We won't make worked examples of all cases. But here is how you can simulate data and then fit it with MCMC for any case.

Simulate new data:

```{r eval=recalculate}
param_nodes <- c('slope','intercept','sigma')
sim_nodes <- linear_model$getDependencies(param_nodes, self=FALSE)
linear_model$intercept <- 5
linear_model$slope <- 0.7
linear_model$sigma <- 0.3
linear_model$simulate(sim_nodes, includeData = TRUE)
linear_model$y
```

Fit the model
```{r eval=recalculate}
samples <- nimbleMCMC(model = linear_model)
summary(samples)
```

An alternative if you are not simulating data is to skip building the model and let `nimbleMCMC` do that, like this:
```{r eval=FALSE}
samples <- nimbleMCMC(linear_model_code,
                      constants = list(N = 10),
                      data = list(y = y, x = x))
```

Handling non-normal data and associated link functions
=====

Option 1:
```{r, eval=FALSE}
nimbleCode({
  # Incomplete code snippet
  for(i in 1:N) {
    logit_y[i] <- intercept + slope*x[i]
    y[i] ~ dbern(expit(logit_y[i])) # Bernoulli distribution
  }
})
```

Option 2:
```{r, eval=FALSE}
nimbleCode({
  # Incomplete code snippet
  for(i in 1:N)
    logit(y[i]) ~ dbern(intercept + slope*x[i])
})
```

(Internally, nimble will turn the second version into the first version.)


Handling factors (groups)
=====
Say we have a factor with 3 levels (groups) and 4 data points for each level.

Say we want a different intercept for each level (group).

### Option 1: Nested indexing

Setup:
```{r, eval=recalculate}
group <- c(1,1,1,1,2,2,2,2,3,3,3,3)
```

```{r, eval=recalculate}
y <- rnorm(12)
x <- rnorm(12)
linear_model_code <- nimbleCode({
  for(i in 1:3)
    intercept[i] ~ dnorm(0, sd = 100) # vague priors
  slope ~ dnorm(mean = 0, sd = 100)
  sigma ~ dunif(min = 0, max = 100)
  for(i in 1:N) {
    y[i] ~ dnorm(intercept[group[i]] + slope*x[i], sd = sigma)
  }
})
linear_model <- nimbleModel(linear_model_code,
                            constants = list(N = 12, group = group, x = x),
                            data = list(y = y))
```

**It is important to put `groups` in `constants`, not `data`**.

Handling factors (groups) (cont)
=====

### Option 2: Dummy variables and model matrix

We will also include `x` in the model matrix.

Somehow make a matrix of dummy variables. I'll do it with `model.matrix`
```{r, eval=recalculate}
X <- model.matrix(y ~ x + as.factor(group) - 1)
attributes(X) <- NULL
dim(X) <- c(12, 4)
X
```

```{r, eval=recalculate}
linear_model_code <- nimbleCode({
  for(i in 1:4)
    beta[i] ~ dnorm(0, sd = 100) # vague priors
  sigma ~ dunif(min = 0, max = 100)
  predicted_y[1:N] <- (X[1:12, 1:4] %*% beta[1:4])[,1]
  for(i in 1:N) {
    y[i] ~ dnorm(predicted_y[i], sd = sigma)
  }
})
linear_model <- nimbleModel(linear_model_code,
                            constants = list(N = 12, X = X),
                            data = list(y = y))

```

**Option 1** and **Option 2** are statistically equivalent but may (will) give different performance in algorithms. For example, MCMCs will be able to do fewer calculations from Option 1 because the model will know exactly which `y[i]` depend on which `intercept[i]`. That will not be the case from the matrix multiplication `(X[1:12, 1:4] %*% beta[1:4])` because that declares a single (non-scalar) relationship.

In either option you can use different *contrasts*. E.g., having a "reference" group leads to different contrasts.

Random effects
=====

Model code for random effects look a lot like that for fixed effects, with the addition of a shared prior.

We can write random effects as **centered** or **uncentered** (or "partially" centered).

Let's say we want the `group` effects above to be random effects. We'll use Option 1 and uncentered random effects.

```{r, eval=recalculate}
linear_model_code <- nimbleCode({
  sigma_beta ~ dhalfflat() #illustrate improper uninformative prior for >0.
  for(i in 1:3)
    group_effect[i] ~ dnorm(0, sd = sigma_beta)
  intercept ~ dnorm(0, sd = 100)
  slope ~ dnorm(mean = 0, sd = 100)
  sigma ~ dunif(min = 0, max = 100)
  for(i in 1:N) {
    y[i] ~ dnorm(intercept + group_effect[group[i]] + slope*x[i], sd = sigma)
  }
})
linear_model <- nimbleModel(linear_model_code,
                            constants = list(N = 12, group = group, x = x),
                            data = list(y = y))
```

Random effects (continued)
=====

Here is a **centered** version of the above example.

```{r, eval=recalculate}
linear_model_code <- nimbleCode({
  sigma_beta ~ dhalfflat() #illustrate improper uninformative prior for >0.
  for(i in 1:3)
    group_effect[i] ~ dnorm(intercept, sd = sigma_beta)
  intercept ~ dnorm(0, sd = 100)
  slope ~ dnorm(mean = 0, sd = 100)
  sigma ~ dunif(min = 0, max = 100)
  for(i in 1:N) {
    y[i] ~ dnorm(group_effect[group[i]] + slope*x[i], sd = sigma)
  }
})

linear_model <- nimbleModel(linear_model_code,
                            constants = list(N = 12, group = group, x = x),
                            data = list(y = y))
```

Random effects (continued)
=====

And here is a fully uncentered version. (The first version might be called "partially" centered.)

```{r, eval=recalculate}
linear_model_code <- nimbleCode({
  sigma_beta ~ dhalfflat() #illustrate improper uninformative prior for >0.
  for(i in 1:3)
    group_effect[i] ~ dnorm(0, sd = 1)
  intercept ~ dnorm(0, sd = 100)
  slope ~ dnorm(mean = 0, sd = 100)
  sigma ~ dunif(min = 0, max = 100)
  for(i in 1:N) {
    y[i] ~ dnorm(intercept + group_effect[group[i]]*sigma_beta + slope*x[i], sd = sigma)
  }
})
linear_model <- nimbleModel(linear_model_code,
                            constants = list(N = 12, group = group, x = x),
                            data = list(y = y))
```

The various ways to write random effects are statistically equivalent but can (will) result in different MCMC mixing and posterior correlations (or performance of other algorithms).

Random efects can also be set up with a model matrix and matrix multiplication. Again that would reduce nimble's capability to determine what is really connected to what. 

And of course one can also set up slopes (or coefficients) as random effects with the same concepts.

Occupancy models (discrete latent states)
=====

Here is a very basic occupancy model with one covariate for detection (`w`) and one for occurrence (`x`)

```{r, eval=recalculate}
occ_code <- nimbleCode({
  for(i in 1:2)
    alpha[i] ~ dnorm(0, sd = 100) # parameters for occurrence
  for(j in 1:2)
    beta[j] ~ dnorm(0, sd = 100) # parameters for detection
  for(i in 1:num_sites) {
    logit(psi[i]) <- alpha[1] + alpha[2]*x[i] 
    z[i] ~ dbern(psi[i])
    for(j in 1:num_visits[i]) { # illustrate indexing on inner for loop range
      logit(p[i,j]) <- beta[1] + beta[2]*w[i, j]
      y[i,j] ~ dbern(z[i] * p[i, j])
    }
  }
})
```

```{r, eval=recalculate}
num_sites <- 100
num_visits <- rep(4, 100)
x <- rnorm(num_sites)
w <- matrix(rnorm(num_sites * max(num_visits)), nrow = num_sites)
# let's use the model to simulate data
occ_model <- nimbleModel(occ_code,
                         constants = list(num_sites = num_sites,
                                          num_visits = num_visits,
                                          w = w,
                                          x = x),
                         calculate=FALSE) # don't calculate because values are not fully set up
param_nodes <- c('alpha[1:2]','beta[1:2]')
sim_nodes <- occ_model$getDependencies(param_nodes, self=FALSE)
occ_model$alpha <- c(0, .2)
occ_model$beta <- c(0.5, -0.2)
occ_model$simulate(sim_nodes)
head(occ_model$y)
head(occ_model$z)
occ_model$setData('y')
```

Occupancy models can include groups and random effects using the ideas above.

State-space models (time-series with process noise and observation error)
=====

We'll use a simple linear auto-regressive state-space model.

```{r, eval=recalculate}
ss_code <- nimbleCode({
  x[1] ~ dnorm(0, sd = 100) # initial state; other approaches are possible.
  for(t in 2:num_times)
    x[t] ~ dnorm(alpha + rho*x[t-1], sd = sigma_process) # auto-regressive state dynamics
  for(t in 1:num_times)
    y[t] ~ dnorm(x[t], sd = sigma_obs) # noisy observations of the states
  alpha ~ dnorm(0, sd = 100)
  rho ~ dunif(-1, 1) 
  sigma_process ~ dhalfflat() #arbitrarily illustrating different priors
  sigma_obs ~ dunif(0, 100)
})
```

```{r, eval=recalculate}
ss_model <- nimbleModel(
  ss_code,
  constants = list(num_times = 20),
  inits = list(alpha = 0, rho = 0.5,
               sigma_process = 0.1, sigma_obs = 0.1,
               x = c(0.1, rep(NA, 19))),
  calculate=FALSE
)
# giving inits is another way to get values in the model initially.
param_nodes <- ss_model$getNodeNames(topOnly=TRUE)
param_nodes
sim_nodes <- ss_model$getDependencies(param_nodes, self=FALSE)
head(sim_nodes)
ss_model$simulate(sim_nodes, includeData = TRUE)
ss_model$y
```

Discrete Markov models, also illustrating constraints
=====

Say `z[t]` is a discrete state taking integer values 1-3. Say `A` is a transition matrix such that `A[i,j]` is the probability of changing from state `i` to `j`. Say `z[t]` is observed correctly.

The elements of `A` might be parameters themselves or might be calculations from other parameters. Each row of `A` should sum to 1 (although `dcat` will normalize to 1 anyway).

If the states correspond to life stages, one state might be "absorbing", i.e. the dead states.

Let's assume the states are locations where an animal may be found, and that probabilities of moving between locations are symmetric.

(The case with imperfect observation, a *Hidden Markov Model* (HMM) is an extension. In that case we would also need a matrix of probabilities of observing each state from each actual state along with a model component for initial state probabilities.)

```{r, eval=recalculate}
mm_code <- nimbleCode({
  prob12 ~ dunif(0,1)
  prob23 ~ dunif(0,1)
  prob13 ~ dunif(0,1)
  ones[1] ~ dconstraint(prob12 + prob13 < 1)
  ones[2] ~ dconstraint(prob12 + prob23 < 1)
  ones[3] ~ dconstraint(prob23 + prob13 < 1)
  A[1,1:3] <- c(1-prob12-prob13, prob12, prob13)
  A[2,1:3] <- c(prob12, 1-prob12-prob23, prob23)
  A[3,1:3] <- c(prob13, prob23, 1-prob13-prob23)  
  for(t in 2:num_times) {
    z[t] ~ dcat(A[z[t-1], 1:3])
  }
})
mm_model <- nimbleModel(mm_code,
                        constants = list(num_times = 20),
                        data = list(ones = rep(1, 3),
                                    z = c(2, rep(NA, 19))), # discuss missing data as NA
                        calculate=FALSE)
mm_model$prob12 <- 0.2
mm_model$prob13 <- 0.3
mm_model$prob23 <- 0.4
param_nodes <- c("prob12", "prob23", "prob13")
sim_nodes <- mm_model$getDependencies(param_nodes, self=FALSE)
mm_model$simulate(sim_nodes)
mm_model$z
mm_model$A
```

Documentation on available functions and distributions
=====

Go to [Documentation](https://r-nimble.org/documentation-2) and discuss.
