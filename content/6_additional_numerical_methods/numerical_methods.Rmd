---
title: "Numerical Methods in NIMBLE"
subtitle: "NIMBLE 2024 TWS Workshop"
author: "NIMBLE Development Team"
date: "October 2024"
output:
  slidy_presentation: default
  html_document:
    code_folding: show
---
<style>
slides > slide {
  overflow-x: auto !important;
  overflow-y: auto !important;
}
</style>

```{r setup, include=FALSE}
library(nimble)
library(nimbleEcology)
has_rjags <- require(rjags)
if(!has_rjags)
  warning("This Rmd file uses package rjags.  Sections using it will be skipped.")
recalculate <- FALSE
makeplot <- !recalculate
```

Laplace approximation
====

- Laplace approximation (LA) is a fast method (often accurate) for approximating definite integrals.
- In statistics, LA is often used to approximate marginal likelihoods / posterior distributions
- When we have random effects, one way to perform maximum likelihood is to marginalize (integrate) out the random effects.
- The problem LA aims to solve is to marginalize a density of observed and latent variables, $\mathbf{y}$ and $\mathbf{u}$,

$$
\tag{1}
[\mathbf{y}|\boldsymbol{\theta}] = \int [\mathbf{y}, \mathbf{u} | \boldsymbol{\theta}] d\mathbf{u} \approx
\frac{[\mathbf{y},\widehat{\mathbf{u}} | \boldsymbol{\theta}]}{g(\mathbf{\widehat{u}}|\mathbf{y}, \boldsymbol{\theta})},
$$
where 

- $\boldsymbol{\theta}$: a vector of model parameters (e.g. variance)
- $\mathbf{u}$: a vector of random effects
- $\mathbf{y}$: a vector of data
- $\widehat{\mathbf{u}}$: MLE of random effects for a given value of $\boldsymbol{\theta}$
- $g(\mathbf{\widehat{u}}|\mathbf{y},\boldsymbol{\theta})$: is a Gaussian approximation of the random effects evaluated at their mean.
 

Laplace approximation
====

We see Laplace approximation used in software such as 

- INLA
- TMB
- glmmTMB
- lme4
- and many more.

Laplace in NIMBLE
====

- In NIMBLE, an LA algorithm can be built using **`buildLaplace`** as follows:
```{r,eval = FALSE }
laplace <- buildLaplace(model, # model object, the only compulsory input
                        paramNodes, # top-level stochastic nodes
                        randomEffectsNodes, # latent variables
                        calcNodes, # random effects and dependencies + necessary deterministic nodes
                        calcNodesOther, # data nodes depending only on params, i.e. y*
                        control = list() # Additional settings, e.g. inner optimiser, checking, Laplace split etc
                        )
```
- Compile the algorithm:
```{r, eval = FALSE}
Claplace <- compileNimble(laplace, project = model)
```
- Run it:
```{r, eval = FALSE}
res <- runLaplace(laplace, # Laplace algorithm
                  pStart,  # Start value for MLE
                  method = "BFGS",               # Outer optimisation method
                  originalScale = TRUE,          # Scale of MLE results: original or transformed (unconstrained)
                  randomEffectsStdError = TRUE,  # Return std. error of random effects?
                  jointCovariance = FALSE        # Return joint covariance matrix of params and random effects?
                  )
```

Occupancy Model with Random Effects
====

Simulate some data;
```{r}
library(nimbleEcology)
set.seed(123)
## number of sites
nLocs <- 5
nSites <- 20
N <- nLocs*nSites
## number of observation periods
T <- 6


## simulate z (occupied status),
## and y (encounter histories)
z <- matrix(NA, nrow = nSites, ncol = nLocs)
y <- array(NA, c(nSites, nLocs, T))
for( i in 1:nSites ){
  pOcc <- expit(-1 + rnorm(1, 0,0.25))
  pObs <- expit(1 + rnorm(1, 0, 0.1))
  for( j in 1:nLocs ) {
      z[i,j] <- rbinom(1, size=1, prob=pOcc)
      y[i, j, 1:T] <- rbinom(T, size=1, prob=z[i,j]*pObs)
  }
}
```

```{r, eval = FALSE}
occmodel <- nimbleCode({
  beta0_det ~ dflat()
  beta0_occ ~ dflat()

  sigma_occ ~ dhalfflat()
  sigma_obs ~ dhalfflat()
  
  for( i in 1:nSites ){
    z_re[i] ~ dnorm(0, sd = sigma_occ)
    pOcc[i] <- expit(beta0_occ + z_re[i])
    y_re[i] ~ dnorm(0, sd = sigma_obs)
    pObs[i] <- expit(beta0_det + y_re[i])
    
    for( j in 1:nLocs ){
      y[i,j,1:T] ~ dOcc_s(pOcc[i], pObs[i], len=T)
    }
  }
})

occ_constants <- list(nSites = nSites, nLocs = nLocs, T = T)

occ_inits <- function(){
  list(sigma_occ = rgamma(1,1,1), sigma_obs = rgamma(1, 1, 1), z_re = rnorm(nrow(z)), y_re = rnorm(nrow(z)), beta0_det = 0, beta_occ = 0)
}

model <- nimbleModel(occmodel, 
                      data = list(y = y), 
                      constants = occ_constants,
                      inits = occ_inits(), buildDerivs = TRUE)
cmodel <- compileNimble(model)
laplace <- buildLaplace(model)

pars <- laplace$getNodeNamesVec()
laplace$calcLogLik(p = values(model, pars))

claplace <- compileNimble(laplace, project = model)
res <- runLaplace(claplace)

## MLE
res$summary$params

## Get random effects
res$summary$randomEffects

values(cmodel, pars)

## Just manually find MLE
claplace$findMLE()

## Find marginalized log likelihood
claplace$calcLaplace(values(cmodel,pars))
```

Some notes about NIMBLE Laplace
====

- Input nodes (except for model code) for Laplace, if not provided or only partially provided, will be decided by **`setupMargNodes`**; see `help("setupMargNodes")`.

- The automatic decision process should be perfect in most cases, but not always. For example, for state-space models the initial state will be treated as a parameter, not random effect. Nead to provide the arguments manually.

- One useful feature is the split of Laplace approximation (set `control = list(split = TRUE)` for `buildLaplace` or `split = TRUE` for `setupMargNodes`).

- For easier (better?) optimisation (both inner and outer), we apply automatic transformations to constrained parameter and/or random effects nodes; see `help("parameterTransform")`. 

- A very recent feature is that `nimOptim` can incorporate additional optimisation methods (e.g. those in `nlminb`). For Laplace, set the inner optimiser using `control = list(innerOptimMethod = "nlminb")` in `buildLaplace`. 

```{r, eval = FALSE}
claplace$updateSettings(innerOptimMethod = "nlminb", useInnerCache = FALSE)
fit.nlminb <- claplace$findMLE(hessian = FALSE)
fit.nlminb$par
```

Additional Options (AGHQ)
====

Adaptive Gauss-Hermite Quadrature (AGHQ) is a generalization of Laplace that includes additional integration nodes. When Laplace is not very accurate, increasing the number of integration points can improve estimation.

Available as `buildAGHQ`, or as part of `Laplace`.

Good check to see if Laplace is accurate.

```{r, eval = FALSE}
claplace$updateSettings(nQuad = 1)
claplace$calcLogLik(p = values(cmodel, pars))

claplace$updateSettings(nQuad = 3)
claplace$calcLogLik(p = values(cmodel, pars))

claplace$updateSettings(nQuad = 11)
claplace$calcLogLik(p = values(cmodel, pars))
```

Additional Options (MCMC)
====

**Option: integrate out all latent states and use MCMC for parameters only**

- Write a custom log-likelihood function
```{r, eval=FALSE}
llFun_Laplace <- nimbleFunction(
  setup = function(model, paramNodes, randomEffectsNodes, calcNodes) {
    laplace <- buildLaplace(model, paramNodes, randomEffectsNodes, calcNodes)
  },
  run = function() {
    pvals <- values(model, paramNodes)
    ll <- laplace$calcLaplace(pvals)
    returnType(double())
    return(ll)
  }
)
randomEffectsNodes <- c("y_re", "z_re")
paramNodes <- pars
calcNodes <- model$getDependencies(randomEffectsNodes)
## Generate the custom log-likelihood function for MCMC
Rll_Laplace <- llFun_Laplace(model, paramNodes, randomEffectsNodes, calcNodes)
```
- Specify a `RW_llFunction` sampler for parameter nodes.

```{r, eval=FALSE}
mcmcConf <- configureMCMC(model, nodes = NULL)
paramNodes <- model$expandNodeNames(paramNodes, returnScalarComponents = TRUE)
for(tarNode in paramNodes){
  mcmcConf$addSampler(target = tarNode, type = "RW_llFunction",
                      control = list(llFunction = Rll_Laplace, includesTarget = FALSE))
}
mcmc <- buildMCMC(mcmcConf)
cmcmc <- compileNimble(mcmc)
fit.mcmc <- runMCMC(cmcmc, niter = 10000, 
                          nchains = 3, nburnin = 5000, samplesAsCodaMCMC = TRUE)
plot(fit.mcmc[,c('beta0_det', 'beta0_occ')])
```


Some notes about Laplace within MCMC
====

- The code to set this up seems a bit but most of the code could be copy-pasted.
- You will need to have a good understanding of your model structure to specify the approximate custom likelihood. 
- The `RW_llFunction` sampler in NIMBLE is a random-walk sampler using custom likelihood (Laplace here) instead of using `model$calculate()`; the multivariate version is `RW_llFunction_block`.
- Other samplers using Laplace/other custom likelihood functions could be set up similarly but will need you to modify the original samplers' code.
- Evaluating a single Laplace approximation is not expensive, but it is a different story when being involved in MCMC running a large number of iterations. Thus the MCMC+Laplace workflow would be ideal for models with high-dimensional and complex random effects structures.


Future Developments for `nimbleQuad`
====

- Additional quadrature rules

More  numerical integration within models. E.g. sparse quadrature grids for when Laplace is ineffective.

- Approximate Bayesian inference through numerical integration (INLA like)

Similar to buildLaplace but instead of the MLE get the maximum a posteriori mode (MAP) and approximate credible intervals.
